{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ElwFwLU2uiQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "t7XOMSHN2Zij"
   },
   "outputs": [],
   "source": [
    "# Implement Continuous Bag of Words (CBOW) Model. Stages can be:\n",
    "# a. Data preparation\n",
    "# b. Generate training data\n",
    "# c. Train model\n",
    "# d. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nVqMzx7U3SQN"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7tqFgbpO3dnF"
   },
   "outputs": [],
   "source": [
    "# Sample corpus and parameters\n",
    "corpus = [\"we are learning the CBOW model\", \"CBOW uses context words to predict target words\"]\n",
    "embedding_dim = 10\n",
    "context_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5_ylJX03r_n"
   },
   "source": [
    "a. data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JQoCcY-F3odV"
   },
   "outputs": [],
   "source": [
    "words = [word for sentence in corpus for word in sentence.split()]\n",
    "vocab = sorted(set(words))\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdJUGGFq31oS"
   },
   "source": [
    "b. generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XisynNpw3vWs"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(context_size, len(words) - context_size):\n",
    "    context = [words[i - j] for j in range(1, context_size + 1)] + [words[i + j] for j in range(1, context_size + 1)]\n",
    "    target = words[i]\n",
    "    data.append(([word_to_index[w] for w in context], word_to_index[target]))\n",
    "\n",
    "X_train = np.array([np.mean([np.eye(vocab_size)[w] for w in x], axis=0) for x, _ in data])\n",
    "y_train = np.array([np.eye(vocab_size)[y] for _, y in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zD6BsIhg36G5"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(embedding_dim, input_shape=(vocab_size,)),\n",
    "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYZ1gfiF53jW"
   },
   "source": [
    " c.train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wkMhf3s6BNZ",
    "outputId": "3068ec03-d2b7-4461-cdec-ecfbe0e53b0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7cff00c696f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit(X_train, y_train, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_9dWi7O5u4I"
   },
   "source": [
    "d.Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZzII_-g44AK6"
   },
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(context):\n",
    "    context_vector = np.mean([np.eye(vocab_size)[word_to_index[w]] for w in context], axis=0).reshape(1, -1)\n",
    "    prediction = model.predict(context_vector)\n",
    "    return vocab[np.argmax(prediction)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CfL4tgC4F3O",
    "outputId": "512424f0-7266-48f5-f157-10d71d4c181d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "CBOW\n"
     ]
    }
   ],
   "source": [
    "# Test prediction\n",
    "print(predict([\"we\", \"learning\", \"the\", \"model\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "knA3dIHJ4J9J"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oaKtLaD4tPk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
